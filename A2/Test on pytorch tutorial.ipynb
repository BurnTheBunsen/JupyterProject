{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-10T23:38:56.831814Z",
     "start_time": "2025-10-10T23:38:55.372677Z"
    }
   },
   "source": [
    "from operator import length_hint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T23:38:56.844164Z",
     "start_time": "2025-10-10T23:38:56.840454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Our raw data, which consists of sentences\n",
    "corpus = [\n",
    "          \"We always come to Paris\",\n",
    "          \"The professor is from Australia\",\n",
    "          \"I live in Stanford\",\n",
    "          \"He comes from Taiwan\",\n",
    "          \"The capital of Turkey is Ankara\"\n",
    "         ]"
   ],
   "id": "d4adf00035f87a03",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T23:38:56.861404Z",
     "start_time": "2025-10-10T23:38:56.853436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### this is the preprocessing steps that we will take for our data\n",
    "def preprocess_sentence(sentence):\n",
    "    return sentence.lower().split()\n",
    "\n",
    "train_sentences = [preprocess_sentence(sent) for sent in corpus]\n",
    "train_sentences"
   ],
   "id": "46293ee5c4763118",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['we', 'always', 'come', 'to', 'paris'],\n",
       " ['the', 'professor', 'is', 'from', 'australia'],\n",
       " ['i', 'live', 'in', 'stanford'],\n",
       " ['he', 'comes', 'from', 'taiwan'],\n",
       " ['the', 'capital', 'of', 'turkey', 'is', 'ankara']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T23:38:56.883614Z",
     "start_time": "2025-10-10T23:38:56.879304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "locations = set(['paris', 'australia', 'stanford', 'taiwan', 'turkey', 'ankara'])\n",
    "\n",
    "train_labels = [[1 if word in locations else 0 for word in sent] for sent in train_sentences]\n",
    "train_labels\n"
   ],
   "id": "c154aef3fcb7105c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1, 0, 1]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T23:38:56.899488Z",
     "start_time": "2025-10-10T23:38:56.888696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocabulary = set(w for s in train_sentences for w in s)\n",
    "vocabulary"
   ],
   "id": "fdb5ad76f0abc656",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'always',\n",
       " 'ankara',\n",
       " 'australia',\n",
       " 'capital',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'from',\n",
       " 'he',\n",
       " 'i',\n",
       " 'in',\n",
       " 'is',\n",
       " 'live',\n",
       " 'of',\n",
       " 'paris',\n",
       " 'professor',\n",
       " 'stanford',\n",
       " 'taiwan',\n",
       " 'the',\n",
       " 'to',\n",
       " 'turkey',\n",
       " 'we'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T23:38:56.920340Z",
     "start_time": "2025-10-10T23:38:56.916811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocabulary.add(\"<unk>\")\n",
    "vocabulary.add(\"<pad>\")"
   ],
   "id": "320b54f42fbe1511",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T23:38:56.931925Z",
     "start_time": "2025-10-10T23:38:56.927320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pad_window(sentence, window_size, pad_token=\"<pad>\"):\n",
    "    window = [pad_token] * window_size\n",
    "    return window + sentence + window\n",
    "\n",
    "window_size = 2\n",
    "pad_window(train_sentences[0], window_size=window_size)"
   ],
   "id": "ef2a80c97436fa42",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '<pad>', 'we', 'always', 'come', 'to', 'paris', '<pad>', '<pad>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T23:38:56.948897Z",
     "start_time": "2025-10-10T23:38:56.942901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We are just converting our vocabularly to a list to be able to index into it\n",
    "# Sorting is not necessary, we sort to show an ordered word_to_ind dictionary\n",
    "# That being said, we will see that having the index for the padding token\n",
    "# be 0 is convenient as some PyTorch functions use it as a default value\n",
    "# such as nn.utils.rnn.pad_sequence, which we will cover in a bit\n",
    "ix_into_words = sorted(list(vocabulary))\n",
    "word_into_ix = {word: ind for ind, word in enumerate(ix_into_words)}\n",
    "word_into_ix"
   ],
   "id": "ef93fdc0ebb30022",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<unk>': 1,\n",
       " 'always': 2,\n",
       " 'ankara': 3,\n",
       " 'australia': 4,\n",
       " 'capital': 5,\n",
       " 'come': 6,\n",
       " 'comes': 7,\n",
       " 'from': 8,\n",
       " 'he': 9,\n",
       " 'i': 10,\n",
       " 'in': 11,\n",
       " 'is': 12,\n",
       " 'live': 13,\n",
       " 'of': 14,\n",
       " 'paris': 15,\n",
       " 'professor': 16,\n",
       " 'stanford': 17,\n",
       " 'taiwan': 18,\n",
       " 'the': 19,\n",
       " 'to': 20,\n",
       " 'turkey': 21,\n",
       " 'we': 22}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T23:38:56.965641Z",
     "start_time": "2025-10-10T23:38:56.961857Z"
    }
   },
   "cell_type": "code",
   "source": "# We are ready to convert our training sentences into a sequence of indices corresponding to each token.",
   "id": "8835192da6ba35ad",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T23:38:56.984675Z",
     "start_time": "2025-10-10T23:38:56.977934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Given a sentence of tokens, return the corresponding indices\n",
    "# def convert_token_to_indices(sentences, word_to_ix):\n",
    "#     indices = []\n",
    "#     for token in sentences:\n",
    "#         if token in word_into_ix:\n",
    "#             index = word_to_ix[token]\n",
    "#         else:\n",
    "#             index = word_into_ix[\"<unk>\"]\n",
    "#         indices.append(index)\n",
    "#     return indices\n",
    "def convert_token_to_indices(sentence, word_to_ix):\n",
    "    return [word_into_ix.get(token, word_into_ix[\"<unk>\"])for token in sentence]\n",
    "\n",
    "example_sentence = [\"we\", \"always\", \"come\", \"to\", \"kuwait\"]\n",
    "example_indices = convert_token_to_indices(example_sentence, word_into_ix)\n",
    "restored_example = [ix_into_words[ind] for ind in example_indices]\n",
    "\n",
    "print(example_indices)\n",
    "print(f\"Original sentence is: {example_sentence}\")\n",
    "print(f\"Going from words to indices: {example_indices}\")\n",
    "print(f\"Going from indices to words: {restored_example}\")"
   ],
   "id": "2fb9f7aefc22ccbe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 2, 6, 20, 1]\n",
      "Original sentence is: ['we', 'always', 'come', 'to', 'kuwait']\n",
      "Going from words to indices: [22, 2, 6, 20, 1]\n",
      "Going from indices to words: ['we', 'always', 'come', 'to', '<unk>']\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T23:38:57.000003Z",
     "start_time": "2025-10-10T23:38:56.994650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example_padded_indices = [convert_token_to_indices(s, word_into_ix) for s in train_sentences]\n",
    "example_padded_indices"
   ],
   "id": "3f497711f73066ec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[22, 2, 6, 20, 15],\n",
       " [19, 16, 12, 8, 4],\n",
       " [10, 13, 11, 17],\n",
       " [9, 7, 8, 18],\n",
       " [19, 5, 14, 21, 12, 3]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T23:38:57.021625Z",
     "start_time": "2025-10-10T23:38:57.012095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_dim = 5\n",
    "embeds = nn.Embedding(len(vocabulary), embedding_dim)\n",
    "list(embeds.parameters())"
   ],
   "id": "17536375c9523592",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 1.4759e+00, -2.4065e-01, -9.6251e-01,  3.2371e-01, -1.9047e+00],\n",
       "         [ 5.2418e-02,  1.1083e+00,  5.7652e-01,  2.0985e-02,  4.8053e-01],\n",
       "         [-5.1035e-01, -1.4210e+00, -1.3285e+00, -1.2246e+00, -5.6465e-01],\n",
       "         [ 1.2930e-02, -1.2087e+00,  5.3275e-01,  1.6090e+00,  7.2514e-01],\n",
       "         [ 1.0790e-01, -2.0799e+00, -2.0972e-01,  4.7126e-01,  3.9881e-03],\n",
       "         [-8.5796e-01, -1.6784e+00, -4.0366e-01, -1.8828e+00, -5.2009e-01],\n",
       "         [-5.6866e-01,  4.9350e-02, -3.3253e-01, -5.4979e-01, -5.7710e-01],\n",
       "         [-6.5453e-01, -1.0045e+00,  6.7280e-01, -9.1633e-01, -1.3520e+00],\n",
       "         [-8.0023e-01,  1.1475e+00, -4.0933e-01, -1.1514e+00, -6.6068e-01],\n",
       "         [-4.7910e-01, -1.5182e+00,  4.0799e-02,  1.5528e+00, -5.3339e-01],\n",
       "         [ 1.6279e-01, -6.7179e-01, -6.7538e-01, -1.0129e+00, -4.1664e-01],\n",
       "         [-1.9126e+00,  1.6517e-02,  6.9508e-01, -4.1973e-01,  8.5616e-02],\n",
       "         [ 1.2379e+00, -1.3403e+00,  1.1769e+00,  5.7889e-01, -9.9888e-01],\n",
       "         [-3.9457e-01,  4.0313e-02,  1.3206e+00, -1.1048e+00,  6.6319e-01],\n",
       "         [ 3.7323e-01, -2.0565e+00, -1.0197e+00, -9.6138e-02,  1.3020e+00],\n",
       "         [-1.4449e+00, -7.1967e-01,  1.0257e-02,  4.3839e-01, -6.1869e-01],\n",
       "         [ 2.0410e+00, -1.0306e-02,  1.3592e+00,  8.6387e-01,  6.1996e-01],\n",
       "         [-1.8401e+00, -2.6049e-02,  2.5143e+00, -5.4551e-01, -1.6617e+00],\n",
       "         [-9.8644e-01, -1.3306e+00, -9.2424e-01,  3.9422e-02,  4.7274e-01],\n",
       "         [-2.3521e-01,  3.5649e-01,  3.4897e-01, -1.0249e+00,  2.0152e+00],\n",
       "         [ 5.4923e-02, -7.4424e-01, -4.0266e-01,  3.4033e-01,  3.2797e-02],\n",
       "         [-4.0658e-01,  1.3438e+00,  3.7614e-01,  1.0807e+00, -1.4898e+00],\n",
       "         [-8.7424e-04,  6.1196e-01,  1.0669e+00, -7.8509e-01,  1.3972e+00]],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T23:38:57.036364Z",
     "start_time": "2025-10-10T23:38:57.030598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#we want to get the lookup tensor for the word Paris\n",
    "index = word_into_ix.get('paris')\n",
    "index_tensor = torch.tensor(index, dtype=torch.long)\n",
    "paris_embed = embeds(index_tensor)\n",
    "paris_embed\n"
   ],
   "id": "1cf26eb6c92cfa4f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4449, -0.7197,  0.0103,  0.4384, -0.6187],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T23:38:57.052604Z",
     "start_time": "2025-10-10T23:38:57.043373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "def custom_collate_fn(batch, window_size,word_into_ix):\n",
    "    # we are going to break out our batch into examples 'x' and labels 'y'\n",
    "    # and then turn them into tensors because nn.utiles.rnn.pad_sequence\n",
    "    # expects tensors as inputs\n",
    "    x, y =zip(*batch)\n",
    "    # we have already designed a function for padding but we're just gonna bring it here as well to have everything in one place\n",
    "    def pad_window(sentence, window_size, pad_token=\"<pad>\"):\n",
    "        window = [pad_token] * window_size\n",
    "        return window + sentence + window\n",
    "\n",
    "    # now we pad our examples\n",
    "    x = [pad_window(s, window_size=window_size) for s in x]\n",
    "\n",
    "    # now we need to turn words in our training examples to indices.\n",
    "    def convert_token_to_indices(sentence, word_to_ix):\n",
    "        return [word_to_ix.get(token, word_to_ix['<unk>']) for token in sentence]\n",
    "\n",
    "    # now we convert the examples themselves into indices\n",
    "    x = [convert_token_to_indices(s, word_into_ix) for s in x]\n",
    "\n",
    "    # now we pad all of our examples so they are all the same length\n",
    "    # because our matrix operations are impossible if they are not the same length\n",
    "    pad_token_ix = word_into_ix[\"<pad>\"]\n",
    "\n",
    "    # pad sequence expects tensors so we make x into a tensor\n",
    "    x = [torch.LongTensor(x_i) for x_i in x]\n",
    "    x_padded = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=pad_token_ix)\n",
    "\n",
    "    # We will also pad the labels. Before we do that, we record the number\n",
    "    # of labels so that we know how many words existed in each example\n",
    "    lengths = [len(label) for label in y]\n",
    "    lengths = torch.LongTensor(lengths)\n",
    "\n",
    "    y = [torch.LongTensor(y_i) for y_i in y]\n",
    "    y_padded = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n",
    "\n",
    "\n",
    "    #we are now ready to return our variables. The order we return our variables\n",
    "    # here will match the oder we read them in our training loop\n",
    "    return x_padded, y_padded, lengths\n",
    "    #this order is super important\n"
   ],
   "id": "f39cad94900821ff",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T23:38:57.072148Z",
     "start_time": "2025-10-10T23:38:57.061738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Params to be passed to the Dataloader\n",
    "data = list(zip(train_sentences, train_labels))\n",
    "batch_size = 2\n",
    "shuffle = True\n",
    "window_size = 2\n",
    "collate_fn = partial(custom_collate_fn, window_size=window_size, word_into_ix=word_into_ix)\n",
    "\n",
    "# Now we instantiate the Dataloader\n",
    "loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "#Go through one loop\n",
    "counter = 0\n",
    "for batched_x, batched_y, batched_lengths in loader:\n",
    "    print(f'Iteration {counter}')\n",
    "    print('Batched Input:')\n",
    "    print(batched_x)\n",
    "    print('Batched Labels:')\n",
    "    print(batched_y)\n",
    "    print('Batched Lengths:')\n",
    "    print(batched_lengths)\n",
    "    print('')\n",
    "    counter += 1"
   ],
   "id": "a511c12068a842e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Batched Input:\n",
      "tensor([[ 0,  0,  9,  7,  8, 18,  0,  0,  0],\n",
      "        [ 0,  0, 19, 16, 12,  8,  4,  0,  0]])\n",
      "Batched Labels:\n",
      "tensor([[0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 1]])\n",
      "Batched Lengths:\n",
      "tensor([4, 5])\n",
      "\n",
      "Iteration 1\n",
      "Batched Input:\n",
      "tensor([[ 0,  0, 22,  2,  6, 20, 15,  0,  0,  0],\n",
      "        [ 0,  0, 19,  5, 14, 21, 12,  3,  0,  0]])\n",
      "Batched Labels:\n",
      "tensor([[0, 0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 1, 0, 1]])\n",
      "Batched Lengths:\n",
      "tensor([5, 6])\n",
      "\n",
      "Iteration 2\n",
      "Batched Input:\n",
      "tensor([[ 0,  0, 10, 13, 11, 17,  0,  0]])\n",
      "Batched Labels:\n",
      "tensor([[0, 0, 0, 1]])\n",
      "Batched Lengths:\n",
      "tensor([4])\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now our model needs to create the windows for each word, make a prediction as to whether the center\n",
    "word is a LOCATION or not"
   ],
   "id": "bd5f200bff5884dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Given that our window_size is N we want out model to make a prediction on every 2N+1 tokens, that is if we have an input with 9 tokens, it should return 5 predictions",
   "id": "86b7c69d0caab809"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T23:41:23.856517Z",
     "start_time": "2025-10-10T23:41:23.847462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print original tensor\n",
    "print(f'Original tensor: ')\n",
    "print(batched_x)\n",
    "print('')\n",
    "\n",
    "# Creat the 2 * 2 + 2 chunks\n",
    "chunk = batched_x.unfold(1, window_size*2 + 1, 1)\n",
    "print(f'Windows: ')\n",
    "print(chunk)"
   ],
   "id": "ea45b7968c61da15",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor: \n",
      "tensor([[ 0,  0, 10, 13, 11, 17,  0,  0]])\n",
      "\n",
      "Windows: \n",
      "tensor([[[ 0,  0, 10, 13, 11],\n",
      "         [ 0, 10, 13, 11, 17],\n",
      "         [10, 13, 11, 17,  0],\n",
      "         [13, 11, 17,  0,  0]]])\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2f1c6c06c98f47a3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
