{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-10T14:38:17.658334Z",
     "start_time": "2025-10-10T14:38:14.731940Z"
    }
   },
   "source": [
    "from operator import length_hint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pprint\n",
    "\n",
    "from A2.SP_24_CS224N_PyTorch_Tutorial import batch_size, shuffle\n",
    "\n",
    "pp = pprint.PrettyPrinter()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T14:38:17.666566Z",
     "start_time": "2025-10-10T14:38:17.663319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Our raw data, which consists of sentences\n",
    "corpus = [\n",
    "          \"We always come to Paris\",\n",
    "          \"The professor is from Australia\",\n",
    "          \"I live in Stanford\",\n",
    "          \"He comes from Taiwan\",\n",
    "          \"The capital of Turkey is Ankara\"\n",
    "         ]"
   ],
   "id": "d4adf00035f87a03",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T14:38:17.681215Z",
     "start_time": "2025-10-10T14:38:17.674212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### this is the preprocessing steps that we will take for our data\n",
    "def preprocess_sentence(sentence):\n",
    "    return sentence.lower().split()\n",
    "\n",
    "train_sentences = [preprocess_sentence(sent) for sent in corpus]\n",
    "train_sentences"
   ],
   "id": "46293ee5c4763118",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['we', 'always', 'come', 'to', 'paris'],\n",
       " ['the', 'professor', 'is', 'from', 'australia'],\n",
       " ['i', 'live', 'in', 'stanford'],\n",
       " ['he', 'comes', 'from', 'taiwan'],\n",
       " ['the', 'capital', 'of', 'turkey', 'is', 'ankara']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T14:38:17.701171Z",
     "start_time": "2025-10-10T14:38:17.694220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "locations = set(['paris', 'australia', 'stanford', 'taiwan', 'turkey', 'ankara'])\n",
    "\n",
    "train_labels = [[1 if word in locations else 0 for word in sent] for sent in train_sentences]\n",
    "train_labels\n"
   ],
   "id": "c154aef3fcb7105c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1, 0, 1]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T14:38:17.720441Z",
     "start_time": "2025-10-10T14:38:17.715886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocabulary = set(w for s in train_sentences for w in s)\n",
    "vocabulary"
   ],
   "id": "fdb5ad76f0abc656",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'always',\n",
       " 'ankara',\n",
       " 'australia',\n",
       " 'capital',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'from',\n",
       " 'he',\n",
       " 'i',\n",
       " 'in',\n",
       " 'is',\n",
       " 'live',\n",
       " 'of',\n",
       " 'paris',\n",
       " 'professor',\n",
       " 'stanford',\n",
       " 'taiwan',\n",
       " 'the',\n",
       " 'to',\n",
       " 'turkey',\n",
       " 'we'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T14:38:17.737762Z",
     "start_time": "2025-10-10T14:38:17.734397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocabulary.add(\"<unk>\")\n",
    "vocabulary.add(\"<pad>\")"
   ],
   "id": "320b54f42fbe1511",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T14:38:17.756391Z",
     "start_time": "2025-10-10T14:38:17.750856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pad_window(sentence, window_size, pad_token=\"<pad>\"):\n",
    "    window = [pad_token] * window_size\n",
    "    return window + sentence + window\n",
    "\n",
    "window_size = 2\n",
    "pad_window(train_sentences[0], window_size=window_size)"
   ],
   "id": "ef2a80c97436fa42",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '<pad>', 'we', 'always', 'come', 'to', 'paris', '<pad>', '<pad>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T14:38:17.773295Z",
     "start_time": "2025-10-10T14:38:17.768354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We are just converting our vocabularly to a list to be able to index into it\n",
    "# Sorting is not necessary, we sort to show an ordered word_to_ind dictionary\n",
    "# That being said, we will see that having the index for the padding token\n",
    "# be 0 is convenient as some PyTorch functions use it as a default value\n",
    "# such as nn.utils.rnn.pad_sequence, which we will cover in a bit\n",
    "ix_into_words = sorted(list(vocabulary))\n",
    "word_into_ix = {word: ind for ind, word in enumerate(ix_into_words)}\n",
    "word_into_ix"
   ],
   "id": "ef93fdc0ebb30022",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<unk>': 1,\n",
       " 'always': 2,\n",
       " 'ankara': 3,\n",
       " 'australia': 4,\n",
       " 'capital': 5,\n",
       " 'come': 6,\n",
       " 'comes': 7,\n",
       " 'from': 8,\n",
       " 'he': 9,\n",
       " 'i': 10,\n",
       " 'in': 11,\n",
       " 'is': 12,\n",
       " 'live': 13,\n",
       " 'of': 14,\n",
       " 'paris': 15,\n",
       " 'professor': 16,\n",
       " 'stanford': 17,\n",
       " 'taiwan': 18,\n",
       " 'the': 19,\n",
       " 'to': 20,\n",
       " 'turkey': 21,\n",
       " 'we': 22}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T14:38:17.788226Z",
     "start_time": "2025-10-10T14:38:17.784261Z"
    }
   },
   "cell_type": "code",
   "source": "# We are ready to convert our training sentences into a sequence of indices corresponding to each token.",
   "id": "8835192da6ba35ad",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T14:38:17.808135Z",
     "start_time": "2025-10-10T14:38:17.801187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Given a sentence of tokens, return the corresponding indices\n",
    "# def convert_token_to_indices(sentences, word_to_ix):\n",
    "#     indices = []\n",
    "#     for token in sentences:\n",
    "#         if token in word_into_ix:\n",
    "#             index = word_to_ix[token]\n",
    "#         else:\n",
    "#             index = word_into_ix[\"<unk>\"]\n",
    "#         indices.append(index)\n",
    "#     return indices\n",
    "def convert_token_to_indices(sentence, word_to_ix):\n",
    "    return [word_into_ix.get(token, word_into_ix[\"<unk>\"])for token in sentence]\n",
    "\n",
    "example_sentence = [\"we\", \"always\", \"come\", \"to\", \"kuwait\"]\n",
    "example_indices = convert_token_to_indices(example_sentence, word_into_ix)\n",
    "restored_example = [ix_into_words[ind] for ind in example_indices]\n",
    "\n",
    "print(example_indices)\n",
    "print(f\"Original sentence is: {example_sentence}\")\n",
    "print(f\"Going from words to indices: {example_indices}\")\n",
    "print(f\"Going from indices to words: {restored_example}\")"
   ],
   "id": "2fb9f7aefc22ccbe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 2, 6, 20, 1]\n",
      "Original sentence is: ['we', 'always', 'come', 'to', 'kuwait']\n",
      "Going from words to indices: [22, 2, 6, 20, 1]\n",
      "Going from indices to words: ['we', 'always', 'come', 'to', '<unk>']\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T14:53:30.230314Z",
     "start_time": "2025-10-10T14:53:30.216703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example_padded_indices = [convert_token_to_indices(s, word_into_ix) for s in train_sentences]\n",
    "example_padded_indices"
   ],
   "id": "3f497711f73066ec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[22, 2, 6, 20, 15],\n",
       " [19, 16, 12, 8, 4],\n",
       " [10, 13, 11, 17],\n",
       " [9, 7, 8, 18],\n",
       " [19, 5, 14, 21, 12, 3]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T14:38:17.853838Z",
     "start_time": "2025-10-10T14:38:17.843376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_dim = 5\n",
    "embeds = nn.Embedding(len(vocabulary), embedding_dim)\n",
    "list(embeds.parameters())"
   ],
   "id": "17536375c9523592",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-1.2748,  0.8922,  0.3010,  0.3080,  0.5173],\n",
       "         [ 0.1768, -1.2304, -0.1921,  0.7793,  0.7772],\n",
       "         [-0.3091,  1.5729,  0.2039,  0.5740,  0.5542],\n",
       "         [-1.5117, -0.1493, -0.1691, -1.7332, -0.6309],\n",
       "         [-0.9219, -1.3908, -0.6302,  0.3732, -1.7481],\n",
       "         [-0.8977, -0.7104, -1.0385, -1.3624,  0.8264],\n",
       "         [ 1.2901,  0.3173,  0.6260, -1.3747, -1.2978],\n",
       "         [ 0.8592,  0.2615,  1.0472,  0.5182,  0.9857],\n",
       "         [ 0.3388,  0.2828, -1.4897, -0.2431, -0.1406],\n",
       "         [ 2.3407, -0.4130, -0.2359,  1.1005,  0.7202],\n",
       "         [-0.5725,  0.1554,  1.0098, -0.2164,  0.8102],\n",
       "         [-0.0843,  0.8028,  0.2297,  0.4454,  0.0695],\n",
       "         [-0.6191, -0.7843, -0.2285, -1.6470,  0.5666],\n",
       "         [-0.9508, -1.3548,  0.8492, -0.5451, -2.3403],\n",
       "         [-0.5199, -1.2582, -0.4965,  0.3955,  1.6974],\n",
       "         [ 1.6818, -1.1755, -0.3067, -0.8472, -1.1329],\n",
       "         [-0.3929, -0.1059,  0.7593, -1.0430, -0.8098],\n",
       "         [ 0.3838, -1.2998, -1.3475, -1.4395, -0.3902],\n",
       "         [ 1.6095, -0.0254,  1.3558,  0.9079, -1.5032],\n",
       "         [-0.3151, -1.1721,  0.9260, -1.6594,  0.5754],\n",
       "         [ 0.9933,  0.2923, -0.0649,  1.1486,  1.4029],\n",
       "         [-0.0293, -0.9019, -0.9530,  1.1123, -0.4712],\n",
       "         [-0.5845, -0.3264,  0.3209, -0.6543,  0.6425]], requires_grad=True)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T14:38:17.869582Z",
     "start_time": "2025-10-10T14:38:17.863808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#we want to get the lookup tensor for the word Paris\n",
    "index = word_into_ix.get('paris')\n",
    "index_tensor = torch.tensor(index, dtype=torch.long)\n",
    "paris_embed = embeds(index_tensor)\n",
    "paris_embed\n"
   ],
   "id": "1cf26eb6c92cfa4f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.6818, -1.1755, -0.3067, -0.8472, -1.1329],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T20:14:56.858577Z",
     "start_time": "2025-10-10T20:14:56.846943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "def custom_collate_fn(batch, window_size,word_into_ix):\n",
    "    # we are going to break out our batch into examples 'x' and labels 'y'\n",
    "    # and then turn them into tensors because nn.utiles.rnn.pad_sequence\n",
    "    # expects tensors as inputs\n",
    "    x, y =zip(*batch)\n",
    "    # we have already designed a function for padding but we're just gonna bring it here as well to have everything in one place\n",
    "    def pad_window(sentence, window_size, pad_token=\"<pad>\"):\n",
    "        window = [pad_token] * window_size\n",
    "        return window + sentence + window\n",
    "\n",
    "    # now we pad our examples\n",
    "    x = [pad_window(s, window_size=window_size) for s in x]\n",
    "\n",
    "    # now we need to turn words in our training examples to indices.\n",
    "    def convert_token_to_indices(sentence, word_to_ix):\n",
    "        return [word_to_ix.get(token, word_to_ix['<unk>']) for token in sentence]\n",
    "\n",
    "    # now we convert the examples themselves into indices\n",
    "    x = [convert_token_to_indices(s, word_into_ix) for s in x]\n",
    "\n",
    "    # now we pad all of our examples so they are all the same length\n",
    "    # because our matrix operations are impossible if they are not the same length\n",
    "    pad_token_ix = word_into_ix[\"<pad>\"]\n",
    "\n",
    "    # pad sequence expects tensors so we make x into a tensor\n",
    "    x = [torch.LongTensor(x_i) for x_i in x]\n",
    "    x_padded = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=pad_token_ix)\n",
    "\n",
    "    # We will also pad the labels. Before we do that, we record the number\n",
    "    # of labels so that we know how many words existed in each example\n",
    "    lengths = [len(label) for label in y]\n",
    "    lengths = torch.LongTensor(lengths)\n",
    "\n",
    "    y = [torch.LongTensor(y_i) for y_i in y]\n",
    "    y_padded = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n",
    "\n",
    "\n",
    "    #we are now ready to return our variables. The order we return our variables\n",
    "    # here will match the oder we read them in our training loop\n",
    "    return x_padded, y_padded, lengths\n",
    "    #this order is super important\n"
   ],
   "id": "f39cad94900821ff",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T20:14:59.409192Z",
     "start_time": "2025-10-10T20:14:59.397409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Params to be passed to the Dataloader\n",
    "data = list(zip(train_sentences, train_labels))\n",
    "batch_size = 2\n",
    "shuffle = True\n",
    "window_size = 2\n",
    "collate_fn = partial(custom_collate_fn, window_size=window_size, word_into_ix=word_into_ix)\n",
    "\n",
    "# Now we instantiate the Dataloader\n",
    "loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "#Go through one loop\n",
    "counter = 0\n",
    "for batched_x, batched_y, batched_lengths in loader:\n",
    "    print(f'Iteration {counter}')\n",
    "    print('Batched Input:')\n",
    "    print(batched_x)\n",
    "    print('Batched Labels:')\n",
    "    print(batched_y)\n",
    "    print('Batched Lengths:')\n",
    "    print(batched_lengths)\n",
    "    print('')\n",
    "    counter += 1"
   ],
   "id": "a511c12068a842e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Batched Input:\n",
      "tensor([[ 0,  0,  9,  7,  8, 18,  0,  0,  0,  0],\n",
      "        [ 0,  0, 19,  5, 14, 21, 12,  3,  0,  0]])\n",
      "Batched Labels:\n",
      "tensor([[0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 1]])\n",
      "Batched Lengths:\n",
      "tensor([4, 6])\n",
      "\n",
      "Iteration 1\n",
      "Batched Input:\n",
      "tensor([[ 0,  0, 19, 16, 12,  8,  4,  0,  0],\n",
      "        [ 0,  0, 10, 13, 11, 17,  0,  0,  0]])\n",
      "Batched Labels:\n",
      "tensor([[0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 1, 0]])\n",
      "Batched Lengths:\n",
      "tensor([5, 4])\n",
      "\n",
      "Iteration 2\n",
      "Batched Input:\n",
      "tensor([[ 0,  0, 22,  2,  6, 20, 15,  0,  0]])\n",
      "Batched Labels:\n",
      "tensor([[0, 0, 0, 0, 1]])\n",
      "Batched Lengths:\n",
      "tensor([5])\n",
      "\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T20:11:20.234601Z",
     "start_time": "2025-10-10T20:11:20.230246Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f6d76f626e239203",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ea45b7968c61da15"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
