{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:21:14.672560Z",
     "start_time": "2025-10-11T18:21:14.669313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pprint\n",
    "\n",
    "\n",
    "\n",
    "pp = pprint.PrettyPrinter()"
   ],
   "id": "4c05ee94a4945bfb",
   "outputs": [],
   "execution_count": 356
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:21:14.686880Z",
     "start_time": "2025-10-11T18:21:14.683358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Our raw data, which consists of sentences\n",
    "corpus = [\n",
    "          \"We always come to Paris\",\n",
    "          \"The professor is from Australia\",\n",
    "          \"I live in Stanford\",\n",
    "          \"He comes from Taiwan\",\n",
    "          \"The capital of Turkey is Ankara\"\n",
    "         ]"
   ],
   "id": "171a1d0afeb149d5",
   "outputs": [],
   "execution_count": 357
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:21:14.697922Z",
     "start_time": "2025-10-11T18:21:14.691922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### this is the preprocessing steps that we will take for our data\n",
    "def preprocess_sentence(sentence):\n",
    "    return sentence.lower().split()\n",
    "\n",
    "train_sentences = [preprocess_sentence(sent) for sent in corpus]\n",
    "train_sentences"
   ],
   "id": "3b29309fda1922b5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['we', 'always', 'come', 'to', 'paris'],\n",
       " ['the', 'professor', 'is', 'from', 'australia'],\n",
       " ['i', 'live', 'in', 'stanford'],\n",
       " ['he', 'comes', 'from', 'taiwan'],\n",
       " ['the', 'capital', 'of', 'turkey', 'is', 'ankara']]"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 358
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:21:14.714366Z",
     "start_time": "2025-10-11T18:21:14.709402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "locations = set(['paris', 'australia', 'stanford', 'taiwan', 'turkey', 'ankara'])\n",
    "\n",
    "train_labels = [[1 if word in locations else 0 for word in sent] for sent in train_sentences]\n",
    "train_labels\n"
   ],
   "id": "cf3136f2862ff783",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1, 0, 1]]"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 359
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:21:14.729632Z",
     "start_time": "2025-10-11T18:21:14.724749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocabulary = set(w for s in train_sentences for w in s)\n",
    "vocabulary"
   ],
   "id": "c9c6bf5f120f9716",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'always',\n",
       " 'ankara',\n",
       " 'australia',\n",
       " 'capital',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'from',\n",
       " 'he',\n",
       " 'i',\n",
       " 'in',\n",
       " 'is',\n",
       " 'live',\n",
       " 'of',\n",
       " 'paris',\n",
       " 'professor',\n",
       " 'stanford',\n",
       " 'taiwan',\n",
       " 'the',\n",
       " 'to',\n",
       " 'turkey',\n",
       " 'we'}"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 360
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:21:14.748627Z",
     "start_time": "2025-10-11T18:21:14.744628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocabulary.add(\"<unk>\")\n",
    "vocabulary.add(\"<pad>\")"
   ],
   "id": "92d7c178eebbc679",
   "outputs": [],
   "execution_count": 361
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:21:14.767725Z",
     "start_time": "2025-10-11T18:21:14.761628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pad_window(sentence, window_size, pad_token=\"<pad>\"):\n",
    "    window = [pad_token] * window_size\n",
    "    return window + sentence + window\n",
    "\n",
    "window_size = 2\n",
    "pad_window(train_sentences[0], window_size=window_size)"
   ],
   "id": "6074e9abd9c6869a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '<pad>', 'we', 'always', 'come', 'to', 'paris', '<pad>', '<pad>']"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 362
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:21:14.784119Z",
     "start_time": "2025-10-11T18:21:14.779679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We are just converting our vocabularly to a list to be able to index into it\n",
    "# Sorting is not necessary, we sort to show an ordered word_to_ind dictionary\n",
    "# That being said, we will see that having the index for the padding token\n",
    "# be 0 is convenient as some PyTorch functions use it as a default value\n",
    "# such as nn.utils.rnn.pad_sequence, which we will cover in a bit\n",
    "ix_to_words = sorted(list(vocabulary))\n",
    "word_to_ix = {word: ind for ind, word in enumerate(ix_to_words)}\n",
    "word_to_ix"
   ],
   "id": "2e9b88b3af169393",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<unk>': 1,\n",
       " 'always': 2,\n",
       " 'ankara': 3,\n",
       " 'australia': 4,\n",
       " 'capital': 5,\n",
       " 'come': 6,\n",
       " 'comes': 7,\n",
       " 'from': 8,\n",
       " 'he': 9,\n",
       " 'i': 10,\n",
       " 'in': 11,\n",
       " 'is': 12,\n",
       " 'live': 13,\n",
       " 'of': 14,\n",
       " 'paris': 15,\n",
       " 'professor': 16,\n",
       " 'stanford': 17,\n",
       " 'taiwan': 18,\n",
       " 'the': 19,\n",
       " 'to': 20,\n",
       " 'turkey': 21,\n",
       " 'we': 22}"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 363
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:21:14.823588Z",
     "start_time": "2025-10-11T18:21:14.808292Z"
    }
   },
   "cell_type": "code",
   "source": "# We are ready to convert our training sentences into a sequence of indices corresponding to each token.",
   "id": "69790cfc5d3f7f95",
   "outputs": [],
   "execution_count": 364
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:21:14.852710Z",
     "start_time": "2025-10-11T18:21:14.849343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Given a sentence of tokens, return the corresponding indices\n",
    "# # def convert_token_to_indices(sentences, word_to_ix):\n",
    "# #     indices = []\n",
    "# #     for token in sentences:\n",
    "# #         if token in word_to_ix:\n",
    "# #             index = word_to_ix[token]\n",
    "# #         else:\n",
    "# #             index = word_to_ix[\"<unk>\"]\n",
    "# #         indices.append(index)\n",
    "# #     return indices\n",
    "# def convert_token_to_indices(sentence, word_to_ix):\n",
    "#     return [word_to_ix.get(token, word_to_ix[\"<unk>\"])for token in sentence]\n",
    "#\n",
    "# example_sentence = [\"we\", \"always\", \"come\", \"to\", \"kuwait\"]\n",
    "# example_indices = convert_token_to_indices(example_sentence, word_to_ix)\n",
    "# restored_example = [ix_to_words[ind] for ind in example_indices]\n",
    "#\n",
    "# print(example_indices)\n",
    "# print(f\"Original sentence is: {example_sentence}\")\n",
    "# print(f\"Going from words to indices: {example_indices}\")\n",
    "# print(f\"Going from indices to words: {restored_example}\")"
   ],
   "id": "6d8084d6de43996",
   "outputs": [],
   "execution_count": 365
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:21:14.873282Z",
     "start_time": "2025-10-11T18:21:14.867706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example_padded_indices = [convert_token_to_indices(s, word_to_ix) for s in train_sentences]\n",
    "example_padded_indices"
   ],
   "id": "cb1dfc7887ed5c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[22, 2, 6, 20, 15],\n",
       " [19, 16, 12, 8, 4],\n",
       " [10, 13, 11, 17],\n",
       " [9, 7, 8, 18],\n",
       " [19, 5, 14, 21, 12, 3]]"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 366
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:21:14.908644Z",
     "start_time": "2025-10-11T18:21:14.901879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_dim = 5\n",
    "embeds = nn.Embedding(len(vocabulary), embedding_dim)\n",
    "list(embeds.parameters())"
   ],
   "id": "20a1d5947c18632e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-2.0596, -0.0735, -1.5079, -0.2774,  0.8163],\n",
       "         [-2.2444, -0.5621,  0.5508, -1.7929, -0.7057],\n",
       "         [-0.0600,  0.0720, -0.2341,  0.8129, -0.3203],\n",
       "         [ 0.8071,  0.3906,  1.8616,  0.4977,  0.4330],\n",
       "         [ 0.6045,  0.4990, -1.0775, -0.2709,  0.4924],\n",
       "         [ 0.5458, -1.1854, -0.3158,  0.3069,  1.2659],\n",
       "         [ 1.5832,  0.5701,  1.8572,  2.6360,  1.9865],\n",
       "         [-0.5471, -1.7537,  0.7713, -0.1700, -1.8573],\n",
       "         [ 0.7872, -0.9673,  0.0487,  0.9399,  0.2434],\n",
       "         [-0.3049, -0.3712, -0.2111,  0.6995,  0.8096],\n",
       "         [-1.0291, -0.0579, -0.1719,  0.8258, -1.4774],\n",
       "         [ 1.1457,  1.4173,  1.1532,  1.6549,  1.0306],\n",
       "         [-2.1529, -0.2197, -0.3605,  0.2895, -0.3643],\n",
       "         [-0.3731,  0.0208, -1.6271,  1.1005,  1.4999],\n",
       "         [ 0.2322,  0.0332,  0.8207,  0.0075, -0.0876],\n",
       "         [-1.2228, -1.1438,  0.1233, -0.9908,  0.2386],\n",
       "         [ 0.2188,  1.1597, -0.3406,  0.2690,  0.9704],\n",
       "         [ 0.1175, -0.2737,  0.9022,  0.5569, -0.9592],\n",
       "         [ 0.5621,  0.4395, -0.6066, -0.4380,  1.0090],\n",
       "         [ 1.0684, -1.1850,  0.4722, -0.5135, -1.4767],\n",
       "         [ 1.0219,  1.2015,  1.1170,  0.8300,  1.4046],\n",
       "         [ 1.2558,  0.8252, -1.2582, -0.3588,  0.3965],\n",
       "         [ 0.1711, -1.0145, -0.0364,  1.4382,  1.0894]], requires_grad=True)]"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 367
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:21:14.927673Z",
     "start_time": "2025-10-11T18:21:14.921642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#we want to get the lookup tensor for the word Paris\n",
    "index = word_to_ix.get('paris')\n",
    "index_tensor = torch.tensor(index, dtype=torch.long)\n",
    "paris_embed = embeds(index_tensor)\n",
    "paris_embed\n"
   ],
   "id": "3761bf56a8063563",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2228, -1.1438,  0.1233, -0.9908,  0.2386],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 368
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:21:14.957741Z",
     "start_time": "2025-10-11T18:21:14.951740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "\n",
    "def custom_collate_fn(batch, window_size, word_to_ix):\n",
    "    # we are going to break out our batch into examples 'x' and labels 'y'\n",
    "    # and then turn them into tensors because nn.utiles.rnn.pad_sequence\n",
    "    # expects tensors as inputs\n",
    "    x, y =zip(*batch)\n",
    "\n",
    "    # we have already designed a function for padding but we're just gonna bring it here as well to have everything in one place\n",
    "    def pad_window(sentence, window_size, pad_token=\"<pad>\"):\n",
    "        window = [pad_token] * window_size\n",
    "        return window + sentence + window\n",
    "\n",
    "    # now we pad our examples\n",
    "    x = [pad_window(s, window_size=window_size) for s in x]\n",
    "\n",
    "    # now we need to turn words in our training examples to indices.\n",
    "    def convert_token_to_indices(sentence, word_to_ix):\n",
    "        return [word_to_ix.get(token, word_to_ix['<unk>']) for token in sentence]\n",
    "\n",
    "    # now we convert the examples themselves into indices\n",
    "    x = [convert_token_to_indices(s, word_to_ix) for s in x]\n",
    "\n",
    "    # now we pad all of our examples so they are all the same length\n",
    "    # because our matrix operations are impossible if they are not the same length\n",
    "    pad_token_ix = word_to_ix[\"<pad>\"]\n",
    "\n",
    "    # pad sequence expects tensors so we make x into a tensor\n",
    "    x = [torch.LongTensor(x_i) for x_i in x]\n",
    "    x_padded = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=pad_token_ix)\n",
    "\n",
    "    # We will also pad the labels. Before we do that, we record the number\n",
    "    # of labels so that we know how many words existed in each example\n",
    "    lengths = [len(label) for label in y]\n",
    "    lengths = torch.LongTensor(lengths)\n",
    "\n",
    "    # y = [ [0]*window_size + list(lbl) + [0]*window_size for lbl in y ]\n",
    "    y = [torch.LongTensor(y_i) for y_i in y]\n",
    "    y_padded = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n",
    "\n",
    "\n",
    "    #we are now ready to return our variables. The order we return our variables\n",
    "    # here will match the oder we read them in our training loop\n",
    "    return x_padded, y_padded, lengths\n",
    "    #this order is super important\n"
   ],
   "id": "ffadd9eec6522406",
   "outputs": [],
   "execution_count": 369
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:21:14.970403Z",
     "start_time": "2025-10-11T18:21:14.963917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parameters to be passed to the DataLoader\n",
    "data = list(zip(train_sentences, train_labels))\n",
    "batch_size = 2\n",
    "shuffle = True\n",
    "window_size = 2\n",
    "collate_fn = partial(custom_collate_fn, window_size=window_size, word_to_ix=word_to_ix)\n",
    "\n",
    "# Instantiate the DataLoader\n",
    "loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "# Go through one loop\n",
    "counter = 0\n",
    "for batched_x, batched_y, batched_lengths in loader:\n",
    "  print(f\"Iteration {counter}\")\n",
    "  print(\"Batched Input:\")\n",
    "  print(batched_x)\n",
    "  print(\"Batched Labels:\")\n",
    "  print(batched_y)\n",
    "  print(\"Batched Lengths:\")\n",
    "  print(batched_lengths)\n",
    "  print(\"\")\n",
    "  counter += 1"
   ],
   "id": "6d937831bddbd2a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Batched Input:\n",
      "tensor([[ 0,  0, 19,  5, 14, 21, 12,  3,  0,  0],\n",
      "        [ 0,  0, 10, 13, 11, 17,  0,  0,  0,  0]])\n",
      "Batched Labels:\n",
      "tensor([[0, 0, 0, 1, 0, 1],\n",
      "        [0, 0, 0, 1, 0, 0]])\n",
      "Batched Lengths:\n",
      "tensor([6, 4])\n",
      "\n",
      "Iteration 1\n",
      "Batched Input:\n",
      "tensor([[ 0,  0, 22,  2,  6, 20, 15,  0,  0],\n",
      "        [ 0,  0,  9,  7,  8, 18,  0,  0,  0]])\n",
      "Batched Labels:\n",
      "tensor([[0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 1, 0]])\n",
      "Batched Lengths:\n",
      "tensor([5, 4])\n",
      "\n",
      "Iteration 2\n",
      "Batched Input:\n",
      "tensor([[ 0,  0, 19, 16, 12,  8,  4,  0,  0]])\n",
      "Batched Labels:\n",
      "tensor([[0, 0, 0, 0, 1]])\n",
      "Batched Lengths:\n",
      "tensor([5])\n",
      "\n"
     ]
    }
   ],
   "execution_count": 370
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now our model needs to create the windows for each word, make a prediction as to whether the center\n",
    "word is a LOCATION or not"
   ],
   "id": "b36e818838410244"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Given that our window_size is N we want out model to make a prediction on every 2N+1 tokens, that is if we have an input with 9 tokens, it should return 5 predictions",
   "id": "290f53bf41009f09"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:21:14.985037Z",
     "start_time": "2025-10-11T18:21:14.980356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print original tensor\n",
    "print(f'Original tensor: ')\n",
    "print(batched_x)\n",
    "print('')\n",
    "\n",
    "# Create the 2 * 2 + 2 chunks\n",
    "chunk = batched_x.unfold(1, window_size*2 + 1, 1)\n",
    "print(f'Windows: ')\n",
    "print(chunk)"
   ],
   "id": "b40f8e07dcd054a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor: \n",
      "tensor([[ 0,  0, 19, 16, 12,  8,  4,  0,  0]])\n",
      "\n",
      "Windows: \n",
      "tensor([[[ 0,  0, 19, 16, 12],\n",
      "         [ 0, 19, 16, 12,  8],\n",
      "         [19, 16, 12,  8,  4],\n",
      "         [16, 12,  8,  4,  0],\n",
      "         [12,  8,  4,  0,  0]]])\n"
     ]
    }
   ],
   "execution_count": 371
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we get to our actual model. We have prepared our data and we are ready to build our model.\n",
    "Now we are going to put it all together here."
   ],
   "id": "7bffcd849a73eb8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:21:15.001979Z",
     "start_time": "2025-10-11T18:21:14.996099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class WordWindowClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, hyperparameters, vocab_size, pad_ix=0):\n",
    "        super(WordWindowClassifier, self).__init__()\n",
    "\n",
    "        \"\"\"Instance variables\"\"\"\n",
    "        self.window_size = hyperparameters[\"window_size\"]\n",
    "        self.embed_dim = hyperparameters[\"embed_dim\"]\n",
    "        self.hidden_dim = hyperparameters[\"hidden_dim\"]\n",
    "        self.freeze_embeddings = hyperparameters[\"freeze_embeddings\"]\n",
    "\n",
    "\n",
    "        \"\"\"Embedding Layer\n",
    "        Takes in a tensor containing embedding indices, and returns the corresponding\n",
    "        embeddings. The output is of dim (number_of_indices * embedding_dim)\n",
    "\n",
    "        If freeze_embeddings is True, set the embedding layer parameters to be non-trainable.\n",
    "        This is useful if we only want the parameters other than the embeddings to change.\n",
    "\n",
    "        \"\"\"\n",
    "        self.embeds = nn.Embedding(vocab_size, self.embed_dim, padding_idx=pad_ix)\n",
    "        if self.freeze_embeddings:\n",
    "            self.embed_layer.weight.requires_grad = False\n",
    "\n",
    "        \"\"\"\n",
    "        Hidden Layer\n",
    "        \"\"\"\n",
    "        full_window_size = 2 * window_size + 1\n",
    "        self.hidden_layer = nn.Sequential(\n",
    "            nn.Linear(full_window_size * self.embed_dim, self.hidden_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \"\"\"\n",
    "        Output layer\n",
    "        \"\"\"\n",
    "        self.output_layer = nn.Linear(self.hidden_dim, 1)\n",
    "        \"\"\"\n",
    "        Probabilities\n",
    "        \"\"\"\n",
    "        self.probabilities = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Let B:= Window-padded sentence length\n",
    "            L:= self.embed_dim\n",
    "            S:= self.window_size\n",
    "            H:= self.hidden_layer\n",
    "        \"\"\"\n",
    "        B, L = inputs.size()\n",
    "        \"\"\"\n",
    "        Reshaping\n",
    "        Takes in a (B, L) LongTensor\n",
    "        Outputs a (B, l~, S) LongTensor\n",
    "        \"\"\"\n",
    "        # First, fet our word windows for each word in out input\n",
    "        token_windows = inputs.unfold(1, 2 * self.window_size + 1, 1)\n",
    "        _, adjusted_length, _ = token_windows.size()\n",
    "        # We use underlines here for the two values that we do not care about here\n",
    "        # They are irrelevant at this step\n",
    "\n",
    "        # Now we do a sanity check on our token windows to make sure everything is working properly\n",
    "        assert token_windows.size() == (B, adjusted_length, 2 * self.window_size + 1)\n",
    "\n",
    "        \"\"\"\n",
    "        Embedding.\n",
    "        Takes in a torch.LongTensor of size (B, L~, S)\n",
    "        Outputs a (B, L~, S, D) FloatTensor\n",
    "        \"\"\"\n",
    "        embedded_windows = self.embeds(token_windows)\n",
    "\n",
    "        \"\"\"\n",
    "        Reshaping.\n",
    "        Takes in a (B, L~, S, D) FloatTensor.\n",
    "        Resizes it into a (B, L~, S*D) FloatTensor.\n",
    "        -1 argument \"infers\" what the last dimension should be based on leftover axis.\n",
    "        \"\"\"\n",
    "        embedded_windows = embedded_windows.view(B, adjusted_length, -1)\n",
    "\n",
    "        \"\"\"\n",
    "        Layer 1.\n",
    "        Takes in a (B, L~, S*D) FloatTensor.\n",
    "        Resizes it into (B, L~, H) FloatTensor.\n",
    "        \"\"\"\n",
    "        layer_1 = self.hidden_layer(embedded_windows)\n",
    "\n",
    "        \"\"\"\n",
    "        Layer 2.\n",
    "        Takes in a (B, L~, H) FloatTensor.\n",
    "        Resizes it into (B, L~, 1) Float Tensor.\n",
    "        \"\"\"\n",
    "        output = self.output_layer(layer_1)\n",
    "\n",
    "        \"\"\"\n",
    "        Softmax.\n",
    "        Takes in a (B, L~, 1) FloatTensor of unnormalized class scores.\n",
    "        Outputs a (B, L~, 1) FloatTensor of (log-)normalized c\n",
    "        \"\"\"\n",
    "        output = self.probabilities(output)\n",
    "        output = output.view(B, -1)\n",
    "\n",
    "        return output\n"
   ],
   "id": "b8cfbdd8a816a2cd",
   "outputs": [],
   "execution_count": 372
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Training\n",
    "Now we are ready to put everything together at last.\n",
    "Start with preparing our data and intializing our model.\n",
    "Then we can intialize our optimizer and define out loss function.\n",
    "And this time instead of using a predefined loss function, we will define our own."
   ],
   "id": "572d897ce58a3608"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:21:15.012895Z",
     "start_time": "2025-10-11T18:21:15.007689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data preparation\n",
    "data = list(zip(train_sentences, train_labels))\n",
    "batch_size = 2\n",
    "shuffle = True\n",
    "window_size = 2\n",
    "collate_fn = partial(custom_collate_fn, window_size=window_size, word_to_ix=word_to_ix)\n",
    "\n",
    "# Instantiate a DataLoader\n",
    "loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "# Now initialize our model\n",
    "#It's useful to put all the model hyperparameters into a dictionary\n",
    "model_hyperparameters = {\n",
    "    \"batch_size\": 4,\n",
    "    \"window_size\": 2,\n",
    "    \"embed_dim\": 25,\n",
    "    \"hidden_dim\": 25,\n",
    "    \"freeze_embeddings\": False\n",
    "}\n",
    "\n",
    "vocab_size = len(word_to_ix)\n",
    "model = WordWindowClassifier(model_hyperparameters, vocab_size)\n",
    "\n",
    "# We need to define our optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#Define a loss function, that computes to binary cross entropy loss\n",
    "def loss_function(batch_outputs, batch_labels, batch_lengths):\n",
    "    #Calculate the loss for the whole batch\n",
    "    bceloss = nn.BCELoss()\n",
    "    loss = bceloss(batch_outputs, batch_labels.float())\n",
    "\n",
    "    # Rescale the loss. Remember that we have used lengths to store the\n",
    "    # number of the words in each training example\n",
    "    loss = loss / batch_lengths.sum().float()\n",
    "\n",
    "    return loss"
   ],
   "id": "2f8d2280c5a996aa",
   "outputs": [],
   "execution_count": 373
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now that want to make our training data into batches and then feed them to our model\n",
    "we need to iterate over the batches too"
   ],
   "id": "500c7d0efd58d2d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:21:15.021092Z",
     "start_time": "2025-10-11T18:21:15.016894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function that will be called in every epoch\n",
    "def train_epoch(loss_function, optimizer, model, loader):\n",
    "    #Keep  track of the total loss for the batch\n",
    "    total_loss = 0\n",
    "    for batch_inputs, batch_labels, batch_lengths in  loader:\n",
    "        #clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        #Run a forward pass\n",
    "        outputs = model.forward(batch_inputs)\n",
    "        #Compute the batch loss\n",
    "        loss = loss_function(outputs, batch_labels, batched_lengths)\n",
    "        #calculate gradients\n",
    "        loss.backward()\n",
    "        #update out parameters\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "# Function containing out main training loop\n",
    "def train(loss_function, optimizer, model, loader, num_epochs=10000):\n",
    "    #Iterate through each epoch and call our train_epoch function\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = train_epoch(loss_function, optimizer, model, loader)\n",
    "        if epoch % 100 == 0: print(epoch_loss)"
   ],
   "id": "a5d8b7e8c5fedafc",
   "outputs": [],
   "execution_count": 374
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T16:41:35.183217Z",
     "start_time": "2025-10-11T16:41:35.180297Z"
    }
   },
   "cell_type": "markdown",
   "source": "# LET THE TRAINING BEGIN!",
   "id": "e3105dcb21b68a92"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:21:16.705788Z",
     "start_time": "2025-10-11T18:21:15.026082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 1000\n",
    "train(loss_function, optimizer, model, loader, num_epochs=num_epochs)"
   ],
   "id": "60340728ff092313",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41429315507411957\n",
      "0.3132731541991234\n",
      "0.22233904898166656\n",
      "0.1418863646686077\n",
      "0.1116438414901495\n",
      "0.07947961986064911\n",
      "0.06458008289337158\n",
      "0.04541182518005371\n",
      "0.0365419602021575\n",
      "0.03033832600340247\n"
     ]
    }
   ],
   "execution_count": 375
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Prediction\n",
    "now we can see how well it can actually predict.\n",
    "We can start by creating our test data."
   ],
   "id": "af8c8c91074a8d5d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:52:37.256001Z",
     "start_time": "2025-10-11T18:52:37.249586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create test sentences\n",
    "test_corpus = [\"She come from Paris\"]\n",
    "test_sentences = [s.lower().split() for s in test_corpus]\n",
    "test_labels = [[0, 0 ,0 ,1]]\n",
    "\n",
    "#Create a test loader\n",
    "test_data = list(zip(test_sentences, test_labels))\n",
    "batch_size = 1\n",
    "shuffle = False\n",
    "window_size = 2\n",
    "collate_fn = partial(custom_collate_fn, window_size=window_size, word_to_ix=word_to_ix)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=False, collate_fn=collate_fn )"
   ],
   "id": "ab188344153c87b8",
   "outputs": [],
   "execution_count": 378
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:56:48.807817Z",
     "start_time": "2025-10-11T18:56:48.801305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for test_instance, labels, _ in test_loader:\n",
    "    outputs = model.forward(test_instance)\n",
    "    print(labels)\n",
    "    print(outputs)"
   ],
   "id": "e98e9219157937ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 1]])\n",
      "tensor([[0.0382, 0.0328, 0.0283, 0.9614]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "execution_count": 379
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "18696b0bcd5bb7ff"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
